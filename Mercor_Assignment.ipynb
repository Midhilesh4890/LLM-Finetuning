{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Midhilesh4890/LLM-Finetuning/blob/main/Mercor_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enabling faster GPUs like A100"
      ],
      "metadata": {
        "id": "jOkQzoH4zlzY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqurckHnOs2e"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check ram utilization of the current run time"
      ],
      "metadata": {
        "id": "TuevA2SSzvie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS9IfBQkO0uJ"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD4peeVvP5DO"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import shutil\n",
        "\n",
        "    # get the path to the folder\n",
        "    folder_path = \"/content/llama3\"\n",
        "\n",
        "    # delete the folder\n",
        "    shutil.rmtree(folder_path)\n",
        "except OSError as e:\n",
        "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import shutil\n",
        "\n",
        "    # get the path to the folder\n",
        "    folder_path = \"/content/\"\n",
        "\n",
        "    # delete the folder\n",
        "    shutil.rmtree(folder_path)\n",
        "except OSError as e:\n",
        "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
      ],
      "metadata": {
        "id": "PqIbnLXXxYyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdZ_DTBLNIBW"
      },
      "outputs": [],
      "source": [
        "# Install necessary dependencies\n",
        "!pip install torch transformers python-dotenv numpy datasets tqdm huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "n0Fx5tZLGE51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting different dataset for checking the performance for various datasets to benchmark"
      ],
      "metadata": {
        "id": "pFBnMlgCIZz8"
      }
    },
    {
      "source": [
        "# Load the MMLU dataset with a random configuration\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "available_configs = ['high_school_european_history', 'business_ethics', 'clinical_knowledge', 'medical_genetics', 'high_school_us_history', 'high_school_physics', 'high_school_world_history', 'virology', 'high_school_microeconomics', 'econometrics', 'college_computer_science', 'high_school_biology', 'abstract_algebra', 'professional_accounting', 'philosophy', 'professional_medicine', 'nutrition', 'global_facts', 'machine_learning', 'security_studies', 'public_relations', 'professional_psychology', 'prehistory', 'anatomy', 'human_sexuality', 'college_medicine', 'high_school_government_and_politics', 'college_chemistry', 'logical_fallacies', 'high_school_geography', 'elementary_mathematics', 'human_aging', 'college_mathematics', 'high_school_psychology', 'formal_logic', 'high_school_statistics', 'international_law', 'high_school_mathematics', 'high_school_computer_science', 'conceptual_physics', 'miscellaneous', 'high_school_chemistry', 'marketing', 'professional_law', 'management', 'college_physics', 'jurisprudence', 'world_religions', 'sociology', 'us_foreign_policy', 'high_school_macroeconomics', 'computer_security', 'moral_scenarios', 'moral_disputes', 'electrical_engineering', 'astronomy', 'college_biology']\n",
        "random_config = random.choice(available_configs)\n",
        "print(f\"Random config: {random_config}\")\n",
        "\n",
        "dataset = load_dataset(\"lukaemon/mmlu\", 'marketing')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MGRLvmM1IAy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloning the official Repo"
      ],
      "metadata": {
        "id": "9bt5D-7VF8EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the official Llama-3-8B repository\n",
        "!git clone https://github.com/meta-llama/llama3.git\n",
        "%cd llama3\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, filename='rmsnorm_logs.txt', filemode='w')"
      ],
      "metadata": {
        "id": "Bgkh-Bd7xmsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modified RMSNorm class with logging\n",
        "\n",
        "\n",
        "* **eps (epsilon)**: A small number added to the denominator to prevent division by zero.\n",
        "\n",
        "*  **scale**: A learnable parameter that scales the normalized input. It's\n",
        "initialized to a vector of ones with the same dimension as the input features, allowing different scaling for each feature.\n",
        "\n",
        "* **layer_name**: This is mainly used for debugging purposes, to help identify which layer is being logged.\n",
        "\n",
        "* **call_count**: Keeps track of how many times the forward method has been called. This is used to restrict the logging to the first 65 calls, which can help in avoiding log clutter during long training or inference sessions.\n",
        "\n",
        "*  **RMS Calculation**: The root mean square of the input tensor is calculated along the last dimension, which typically corresponds to feature dimensions in batched input."
      ],
      "metadata": {
        "id": "guvwQrUNBLDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm0BkLf_NsuA"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim, layer_name, eps=1e-8):\n",
        "        super().__init__()\n",
        "        # epsilon to avoid division by zero during normalization\n",
        "        self.eps = eps\n",
        "        # learnable scaling parameter initialized to ones\n",
        "        self.scale = torch.nn.Parameter(torch.ones(dim))\n",
        "        # name of the layer, useful for debugging and tracking\n",
        "        self.layer_name = layer_name\n",
        "        # counter to track the number of times the forward pass is called\n",
        "        self.call_count = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Increment call counter at each forward pass\n",
        "        self.call_count += 1\n",
        "        # Limit logging to the first 65 calls to avoid excessive logging in long-running processes\n",
        "        if self.call_count <= 65:\n",
        "            # Compute the root mean square of the input tensor along the last dimension\n",
        "            rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n",
        "            # Log information about the layer, call number, and RMS value\n",
        "            logging.info(f'Layer: {self.layer_name}, Call: {self.call_count}, RMS: {rms}')\n",
        "        # Normalize the input tensor and scale it\n",
        "        return x / rms * self.scale\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replacing original RMSNorm with Custom RMS function\n",
        "\n",
        "**Iteration through modules**: The function iterates over all modules in the model using model.named_modules(), yielding both the name and module for every component.\n",
        "\n",
        "**Redundant module check**: It checks if the current module is a PyTorch module, which is unnecessary since named_modules() already ensures this.\n",
        "\n",
        "**Attribute iteration**: For each module, the function iterates through all attributes using dir(module) to identify if any are instances of torch.nn.LayerNorm.\n",
        "\n",
        "**Identification of LayerNorm**: The function targets attributes that are torch.nn.LayerNorm instances, essential for the intended replacement with RMSNorm.\n",
        "\n",
        "**LayerNorm replacement**: When a LayerNorm is found, it's replaced with RMSNorm, initialized with dimensions from the original LayerNorm and named by combining the parent's name with its own."
      ],
      "metadata": {
        "id": "XQKXGcHQCu9t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcSf7sGYONaq"
      },
      "outputs": [],
      "source": [
        "# Function to replace RMSNorm layers in the model\n",
        "def replace_rmsnorm(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Module):\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, torch.nn.Module) and isinstance(attr, torch.nn.LayerNorm):\n",
        "                    setattr(module, attr_name, RMSNorm(attr.normalized_shape[0], name + '.' + attr_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Hugging using Token\n",
        "\n",
        "*   this token is saved in .env file in root folder\n",
        "\n"
      ],
      "metadata": {
        "id": "3MMl6gi2DEek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PADuR-NRqU4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import HfApi, notebook_login\n",
        "\n",
        "load_dotenv(\"/content/.env\")\n",
        "HFTOKEN = os.environ.get(\"HFTOKEN\")\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download weights from the repo using Huggingface CLI"
      ],
      "metadata": {
        "id": "Iv6q35MMDgm1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxdivnafYjbS"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download meta-llama/Meta-Llama-3-8B --include \"original/*\" --local-dir Meta-Llama-3-8B"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli download meta-llama/Meta-Llama-3-8B --local-dir Meta-Llama-3-8B"
      ],
      "metadata": {
        "id": "ufA3vEgVuwOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: If config.json is not downloaded along with the original file then use this to download or run above cell."
      ],
      "metadata": {
        "id": "yo44GNkbDo7W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM0YHxdlbPkK"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "  \"text-generation\",\n",
        "  model=model_id,\n",
        "  model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "  device=\"cuda\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the llama3-8b model"
      ],
      "metadata": {
        "id": "8jEr8RZ3EC3b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODgv_4h7NVM2"
      },
      "outputs": [],
      "source": [
        "# Load the Llama-3-8B model inf float 16 precision\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5w-Yw9fR1bX"
      },
      "outputs": [],
      "source": [
        "# Replace the original RMSNorm in the model with the modified version\n",
        "replace_rmsnorm(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Model to Evaluation Mode:** Disables certain layers like dropout and batch normalization that behave differently during training.\n",
        "\n",
        "**Input Preparation:** Concatenates the question and choices into a single string for the model to process.\n",
        "\n",
        "**Tokenization:** Converts the input string into model-readable tokens, adds necessary formatting, and moves the data to the device (GPU or CPU) where the model is located.\n",
        "\n",
        "**Prediction Generation:** The model generates an answer based on the input tokens.\n",
        "\n",
        "**Accuracy Calculation:** The function computes the accuracy by dividing the number of correct predictions by the total number of questions."
      ],
      "metadata": {
        "id": "62Ha7vaIEcjg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-geXy76lNSXa"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_mmlu(model, tokenizer, dataset):\n",
        "    # Set the model to evaluation mode which turns off layers like dropout\n",
        "    model.eval()\n",
        "    correct = 0  # Counter for correct predictions\n",
        "    total = 0    # Counter for total predictions\n",
        "\n",
        "    # Iterate through each example in the test set of the dataset\n",
        "    for example in tqdm(dataset[\"test\"]):\n",
        "        # Prepare the input text by appending question and choices\n",
        "        input_text = example[\"question\"] + \" \" + \" \".join(example[\"choices\"])\n",
        "        # Tokenize the input text and convert it to a tensor suitable for the model\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "        # Generate the answer using the model\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_length=256)\n",
        "        # Decode the generated tensor to a string answer\n",
        "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Check if the predicted answer matches the correct answer\n",
        "        if prediction == example[\"answer\"]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "    # Calculate the accuracy as the ratio of correct answers to total answers\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the model and print the accuracy\n",
        "accuracy = evaluate_mmlu(model, tokenizer, dataset)\n",
        "print(f\"MMLU 5-shot performance with logging: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzpLnvXQNQLB"
      },
      "outputs": [],
      "source": [
        "# Read the log file and extract RMS(a) values\n",
        "with open('rmsnorm_logs.txt', 'r') as f:\n",
        "    rms_logs = [line.strip().split(', ') for line in f if line.startswith('Layer:')]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data for Plotting the values for layers, call_counts and rms values"
      ],
      "metadata": {
        "id": "Ibhj1FlrFKDg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U2fwJaoObC5"
      },
      "outputs": [],
      "source": [
        "# Extract data for plotting\n",
        "layer_names = [log[0].split(': ')[1] for log in rms_logs]\n",
        "call_counts = [int(log[1].split(': ')[1]) for log in rms_logs]\n",
        "rms_values = [float(log[2].split(': ')[1][1:-1]) for log in rms_logs]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histograms"
      ],
      "metadata": {
        "id": "uftG9A48FQWo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jixkttBOc6x"
      },
      "outputs": [],
      "source": [
        "# Create histograms\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(rms_values, bins=50, alpha=0.75, color='blue', edgecolor='black')\n",
        "plt.title('Histogram of RMS(a) values for all layers')\n",
        "plt.xlabel('RMS(a) value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the layers"
      ],
      "metadata": {
        "id": "APUQm04MFSn2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH0sfDCAOfAI"
      },
      "outputs": [],
      "source": [
        "|# Histograms for specific layers\n",
        "layers_to_plot = ['first_rmsnorm_layer', 'last_rmsnorm_layer', 'other_interesting_layer']  # Replace with actual layer names\n",
        "\n",
        "for layer in layers_to_plot:\n",
        "    layer_rms_values = [rms for rms, name in zip(rms_values, layer_names) if name == layer]\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(layer_rms_values, bins=50, alpha=0.75, color='blue', edgecolor='black')\n",
        "    plt.title(f'Histogram of RMS(a) values for {layer}')\n",
        "    plt.xlabel('RMS(a) value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "431hkD7_OhD3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO+awTYUPiJobCTOkT8CBRZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}